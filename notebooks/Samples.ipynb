{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a344b158-535c-45ed-b9fd-b1155eba528c",
   "metadata": {
    "id": "a344b158-535c-45ed-b9fd-b1155eba528c",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# !pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336f9a5e-96c0-4519-8ba9-18238c5efb40",
   "metadata": {},
   "source": [
    "# Índice\n",
    "*Notebook contendo apenas algumas funções basicas*\n",
    "\n",
    "1. [Imports](#Import)\n",
    "2. [Usando dropna](#dropna)\n",
    "3. [Dropando duplicados](#dropDuplicates)\n",
    "4. [Função Sample](#sample)\n",
    "5. [Filtrando com filter ou where](#filter_where)\n",
    "6. [Joins](#Join)\n",
    "7. [Agregação(agg)](#agg)\n",
    "8. [Extras](#Extras)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44712dc-4c0a-47ae-8de6-01af4c1c4a32",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a41a4b0-603f-47d7-970b-fff96e5553e0",
   "metadata": {
    "executionInfo": {
     "elapsed": 376,
     "status": "ok",
     "timestamp": 1740080702647,
     "user": {
      "displayName": "Caio César",
      "userId": "07631910079273269070"
     },
     "user_tz": 180
    },
    "id": "8a41a4b0-603f-47d7-970b-fff96e5553e0"
   },
   "outputs": [],
   "source": [
    "# Importação das bibliotecas necessárias\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, lit, concat, round, substring, udf, length\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ee7e94-194f-454b-b8e6-fbcb45c0ff08",
   "metadata": {
    "executionInfo": {
     "elapsed": 8472,
     "status": "ok",
     "timestamp": 1740080716139,
     "user": {
      "displayName": "Caio César",
      "userId": "07631910079273269070"
     },
     "user_tz": 180
    },
    "id": "e5ee7e94-194f-454b-b8e6-fbcb45c0ff08"
   },
   "outputs": [],
   "source": [
    "# Inicializar uma sessão do Spark e impedindo avisos desnecessários\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NomeDaApp\") \\\n",
    "    .config(\"spark.logConf\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31d5242-34ae-4896-82c1-e7266a5cee57",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## dropna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cdf762-bed3-4ea2-b192-5107c7da6b81",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11583,
     "status": "ok",
     "timestamp": 1740080729660,
     "user": {
      "displayName": "Caio César",
      "userId": "07631910079273269070"
     },
     "user_tz": 180
    },
    "id": "93cdf762-bed3-4ea2-b192-5107c7da6b81",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "062ea2c2-3ec0-4373-c4d6-cb4fab36e491"
   },
   "outputs": [],
   "source": [
    "# Exemplo de DataFrame com 10 registros e 4 campos (incluindo valores nulos/vazios)\n",
    "data = [\n",
    "    (\"Alice\", 34, \"São Paulo\", \"Brasil\"),\n",
    "    (\"Bob\", 45, \"Nova York\", \"EUA\"),\n",
    "    (\"Alice\", None, \"São Paulo\", \"Brasil\"),  # Idade nula\n",
    "    (\"Carla\", 29, \"\", \"Portugal\"),          # Cidade vazia\n",
    "    (\"David\", 30, \"Berlim\", None),          # País nulo\n",
    "    (\"Eva\", 25, \"Paris\", \"França\"),\n",
    "    (\"Bob\", 45, \"Nova York\", \"EUA\"),\n",
    "    (\"Frank\", 40, \"Tóquio\", \"Japão\"),\n",
    "    (\"Grace\", 35, None, \"Reino Unido\"),     # Cidade nula\n",
    "    (\"Hank\", 50, \"Sydney\", \"Austrália\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"Nome\", \"Idade\", \"Cidade\", \"País\"])\n",
    "\n",
    "# Remover linhas com valores nulos ou vazios e salvar na mesma variável `df`\n",
    "df = df.na.drop()  # Remove linhas com qualquer valor nulo ou vazio\n",
    "\n",
    "# Mostrar o resultado\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d024e751-251d-47ab-b6d1-3975462700f4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## dropDuplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KYdtnett5TdY",
   "metadata": {
    "id": "KYdtnett5TdY",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Exemplo de DataFrame com 10 registros e 4 campos\n",
    "data = [\n",
    "    (\"Alice\", 34, \"São Paulo\", \"Brasil\"),\n",
    "    (\"Bob\", 45, \"Nova York\", \"EUA\"),\n",
    "    (\"Alice\", 34, \"São Paulo\", \"Brasil\"),\n",
    "    (\"Carla\", 29, \"Lisboa\", \"Portugal\"),\n",
    "    (\"David\", 30, \"Berlim\", \"Alemanha\"),\n",
    "    (\"Eva\", 25, \"Paris\", \"França\"),\n",
    "    (\"Bob\", 45, \"Nova York\", \"EUA\"),\n",
    "    (\"Frank\", 40, \"Tóquio\", \"Japão\"),\n",
    "    (\"Grace\", 35, \"Londres\", \"Reino Unido\"),\n",
    "    (\"Hank\", 50, \"Sydney\", \"Austrália\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"Nome\", \"Idade\", \"Cidade\", \"País\"])\n",
    "\n",
    "# Remover linhas duplicadas e salvar na mesma variável `df`\n",
    "df = df.dropDuplicates()\n",
    "\n",
    "# Mostrar o resultado\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b912c447-4e6e-45aa-9c20-f1a744a3dfad",
   "metadata": {
    "id": "42c53f2b-5fb8-4bf7-a26e-ea3a8054a796",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd6e140-fc95-477b-b2a3-9c233dac1981",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Dados de exemplo\n",
    "data = [\n",
    "    (\"Alice\", 25, \"São Paulo\"),\n",
    "    (\"Bob\", 30, \"Nova York\"),\n",
    "    (\"Carla\", 22, \"Lisboa\"),\n",
    "    (\"David\", 35, \"Berlim\"),\n",
    "    (\"Eva\", 28, \"Paris\"),\n",
    "    (\"Frank\", 40, \"Tóquio\"),\n",
    "    (\"Grace\", 33, \"Londres\"),\n",
    "    (\"Hank\", 50, \"Sydney\"),\n",
    "    (\"Ivy\", 27, \"Toronto\"),\n",
    "    (\"Jack\", 29, \"Cidade do México\")\n",
    "]\n",
    "\n",
    "# Criar DataFrame\n",
    "df = spark.createDataFrame(data, [\"Nome\", \"Idade\", \"Cidade\"])\n",
    "\n",
    "# Aplicar sample para retornar uma amostra aleatória de 30% dos registros\n",
    "df = df.sample(fraction=0.3, seed=None)\n",
    "\n",
    "# Exibir a amostra\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f9c764-9e90-43b1-859c-12b6ffcfa307",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## filter_where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f9e455-1eb3-414b-8995-85bba7e09c47",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Dados de exemplo\n",
    "data = [\n",
    "    (\"Alice\", 25, \"São Paulo\"),\n",
    "    (\"Bob\", 30, \"Nova York\"),\n",
    "    (\"Carla\", 22, \"Lisboa\"),\n",
    "    (\"David\", 35, \"Berlim\"),\n",
    "    (\"Eva\", 28, \"Paris\"),\n",
    "    (\"Frank\", 40, \"Tóquio\"),\n",
    "    (\"Grace\", 33, \"Londres\"),\n",
    "    (\"Hank\", 50, \"Sydney\"),\n",
    "    (\"Ivy\", 27, \"Toronto\"),\n",
    "    (\"Jack\", 29, \"Cidade do México\")\n",
    "]\n",
    "\n",
    "# Criar DataFrame\n",
    "df = spark.createDataFrame(data, [\"Nome\", \"Idade\", \"Cidade\"])\n",
    "\n",
    "# Filtra linhas com filter onde a coluna \"Idade\" é maior que 30\n",
    "print(\"Uso do filter\")\n",
    "df.filter(df[\"Idade\"] > 30).show()\n",
    "\n",
    "print(\"Uso do where\")\n",
    "df.where(df[\"Idade\"] > 30).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f9e3b3-2dee-49ec-ad3e-64ebb7175919",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5a0f88-3b34-4ab2-827d-b1687fcef8f0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Criando dois dataframes para que possam ser ligados pelo id da cidade\n",
    "data_pessoas = [\n",
    "    (1, \"Alice\", 1),\n",
    "    (2, \"Bob\", 2),\n",
    "    (3, \"Carla\", 3),\n",
    "    (4, \"David\", 5)   \n",
    "]\n",
    "df_pessoas = spark.createDataFrame(data_pessoas, [\"id_pessoa\", \"nome\", \"id_cidade\"])\n",
    "df_pessoas.show()\n",
    "\n",
    "data_cidades = [\n",
    "    (1, \"São Paulo\"),\n",
    "    (2, \"Nova York\"),\n",
    "    (3, \"Lisboa\"),\n",
    "    (4, \"Berlim\")\n",
    "]\n",
    "df_cidades = spark.createDataFrame(data_cidades, [\"id_cidade\", \"cidade\"])\n",
    "df_cidades.show()\n",
    "\n",
    "# Realizando join\n",
    "df_join = df_pessoas.join(df_cidades, df_pessoas.id_cidade == df_cidades.id_cidade, how=\"inner\")\n",
    "# Retornando resultado com select\n",
    "df_join.select(\"id_pessoa\", \"nome\", \"cidade\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7638385-7dc7-4733-b67b-3ce401c00de1",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab48678-8c9d-47bc-b2d3-0e20461295d8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69de19aa-3938-4478-b2d6-d4f59075f790",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Dados de exemplo\n",
    "data = [\n",
    "    (\"2024-11-26\", 654.0, 38.9),\n",
    "    (\"2024-12-03\", 602.1, 40.19),\n",
    "    (\"2024-12-18\", 649.5, 42.80),\n",
    "    (\"2025-01-26\", 636.9, 42.13)\n",
    "]\n",
    "\n",
    "# Criar DataFrame\n",
    "df = spark.createDataFrame(data, [\"data_abastecimento\", \"trip\", \"gasto_gasolina\"])\n",
    "\n",
    "# Usar agg para calcular o gasto de combustivel e criar uma nova coluna\n",
    "df = df.withColumn(\"consumo_total\", round(col(\"trip\") / col(\"gasto_gasolina\"), 2))\n",
    "\n",
    "# Exibir o resultado\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf42882e-aaf7-4854-a1d1-1261968749a5",
   "metadata": {
    "id": "d1a0d836-90fb-49f9-8fd0-db8e2a87db40"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b431db-ab6a-482e-b20e-891741004834",
   "metadata": {
    "id": "547fa663-67d9-43be-96af-a13331998467"
   },
   "source": [
    "## Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b790c50-0341-44eb-ba7a-47ebccf71053",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Caracteres Especiais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409dd5ad-1c46-4ca8-a749-0b59959ee592",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#### Função para checar caracteres especiais e retornar onde se encontram(util em dataframes grandes)\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col,lit\n",
    "from pyspark.sql.types import ArrayType, StringType, StructType, StructField, TimestampType\n",
    "from pyspark.sql.functions import collect_list, concat_ws\n",
    "\n",
    "def search_dataframe(df, tabela):\n",
    "\n",
    "    intermediate_dfs = []\n",
    "\n",
    "    for coluna in df.columns:\n",
    "\n",
    "        processing_df = df.filter(F.lower(df[coluna]).rlike('[^a-z0-9\\s!`´\"#$%&\\'()*+,-./\\]\\[|:;<=>?@\\[\\\\áàâäãéèêëíìîïóòôöõúùûüñç]'))\n",
    "        processing_df = processing_df.withColumn('COLUMN_ERROR', lit(coluna))\\\n",
    "            .withColumn('TABELA', lit(tabela))\n",
    "        intermediate_dfs.append(processing_df)\n",
    "\n",
    "    # Combine todos os DataFrames intermediários em um único DataFrame\n",
    "    result_df = intermediate_dfs[0]\n",
    "    for i in range(1, len(intermediate_dfs)):\n",
    "        result_df = result_df.union(intermediate_dfs[i])\n",
    "\n",
    "    colunas_referencia = df.columns\n",
    "    # Exiba o DataFrame result_df\n",
    "    # result_df = result_df.groupBy(df.columns,\"TABELA\"]).agg(concat_ws(', ',collect_list(\"COLUMN_ERROR\")).alias('COLUMN_ERROR'))\n",
    "    result_df = result_df.groupBy(*[col(coluna) for coluna in colunas_referencia] + [col(\"TABELA\")]).agg(concat_ws(', ',collect_list(\"COLUMN_ERROR\")).alias('COLUMN_ERROR'))\n",
    "\n",
    "    # Retonando dataframe criado dataframe usado + coluna da tabela + coluna(s) com erros\n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb4e748-a6a5-481d-93c4-32eb743a3c75",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"Alice☺\", 25, \"São Paulo\", \"Brasil\", \"Rua A\"),\n",
    "    (\"Bob\", 30, \"Nova York♠\", \"EUA\", \"Rua B\"),\n",
    "    (\"Carla\", 22, \"Lisboa\", \"Portugal✓\", \"Rua C\"),\n",
    "    (\"David\", 35, \"Berlim∞\", \"Alemanha\", \"Rua D\"),\n",
    "    (\"Eva\", 28, \"Paris\", \"França\", \"Rua E\"),\n",
    "    (\"Frank\", 40, \"Tóquio☺\", \"Japão\", \"Rua F\"),\n",
    "    (\"Grace\", 33, \"Londres♠\", \"Reino Unido\", \"Rua G\"),\n",
    "    (\"Hank\", 50, \"Sydney\", \"Austrália✓\", \"Rua H\"),\n",
    "    (\"Ivy\", 27, \"Toronto\", \"Canadá\", \"Rua I\"),\n",
    "    (\"Jack®\", 29, \"Cidade do México\", \"México\", \"Rua J\")\n",
    "]\n",
    "\n",
    "# Nomes das colunas\n",
    "colunas = [\"Nome\", \"Idade\", \"Cidade\", \"País\", \"Rua\"]\n",
    "\n",
    "# Criar DataFrame\n",
    "df = spark.createDataFrame(data, colunas)\n",
    "\n",
    "tabela = 'tabela_teste'\n",
    "resultado = search_dataframe(df,tabela)\n",
    "if resultado.isEmpty():\n",
    "    print(f'Sem dados invalidos na tabela {tabela}')\n",
    "else:\n",
    "    resultado.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682ee28c-65fc-4f8e-af05-6935347c9731",
   "metadata": {},
   "source": [
    "### Calculo_CNPJ\n",
    "*Sabemos que os digitos verificadores do CNPJ, são na verdade um calculo dos numeros anteriores, algumas bases antigas usavam essa estrategia para reduzir custos de armazenamento, deixando que funções proprias calculassem exibido na consulta com os digitos verificadores*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c31c91a-ab44-4e40-8028-5f340bd8daf3",
   "metadata": {},
   "source": [
    "Segue um ex de como deve ser o calculo, Fonte: https://www.cadcobol.com.br/calcula_cpf_cnpj_caepf.htm\n",
    "```\n",
    "Veja, abaixo, exemplo de cálculo de DV módulo 11 (o mais usado pelos bancos) e de DV módulo 10 para o CNPJ nº 18781203/0001:\n",
    "\n",
    "1    8    7    8   1   2   0    3   0   0   0   1               \n",
    "x    x    x    x   x   x   x    x   x   x   x   x               \n",
    "6    7    8    9   2   3   4    5   6   7   8   9            \n",
    "-   --   --   --   -   -   -  ---   -   -   -   -                   \n",
    "6 + 56 + 56 + 72 + 2 + 6 + 0 + 15 + 0 + 0 + 0 + 9 = 222 ÷ 11 = 20, com resto 2\n",
    "\n",
    "\n",
    "1    8    7    8   1   2   0    3   0   0   0   1    2\n",
    "x    x    x    x   x   x   x    x   x   x   x   x    x\n",
    "5    6    7    8   9   2   3    4   5   6   7   8    9\n",
    "-   --   --   --   -   -   -   --   -   -   -   -   --\n",
    "5 + 48 + 49 + 64 + 9 + 4 + 0 + 12 + 0 + 0 + 0 + 8 + 18 = 217 ÷ 11 = 19, com resto 8\n",
    "\n",
    "Portanto, CNPJ+DV = 18.781.203/0001-28\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "245d8752-42b9-415b-8272-32ba34ff47a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|nome_empresa|        cnpj|\n",
      "+------------+------------+\n",
      "|   Empresa A|123456780001|\n",
      "|   Empresa F|789321450002|\n",
      "|   Empresa B|987654320001|\n",
      "|   Empresa C|456789120002|\n",
      "|   Empresa G|852963740001|\n",
      "|   Empresa H|963852140001|\n",
      "|   Empresa D|321654980001|\n",
      "|   Empresa E|654123780001|\n",
      "|   Empresa I|741852960003|\n",
      "|   Empresa J|187812030001|\n",
      "+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (\"Empresa A\", \"123456780001\"),\n",
    "    (\"Empresa B\", \"987654320001\"),\n",
    "    (\"Empresa C\", \"456789120002\"),\n",
    "    (\"Empresa D\", \"321654980001\"),\n",
    "    (\"Empresa E\", \"654123780001\"),\n",
    "    (\"Empresa F\", \"789321450002\"),\n",
    "    (\"Empresa G\", \"852963740001\"),\n",
    "    (\"Empresa H\", \"963852140001\"),\n",
    "    (\"Empresa I\", \"741852960003\"),\n",
    "    (\"Empresa J\", \"187812030001\")\n",
    "]\n",
    "\n",
    "# Criar DataFrame com a ordem desejada\n",
    "df = spark.createDataFrame(data, [\"nome_empresa\", \"cnpj\"])\n",
    "\n",
    "tabela=\"empresas\"\n",
    "df.write.mode(\"overwrite\").saveAsTable(tabela)\n",
    "spark.sql(\"SELECT * FROM empresas\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a6a440b1-089b-43ca-b468-ff65e947285a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+------------------+\n",
      "|nome_empresa|cnpj        |digito_verificador|\n",
      "+------------+------------+------------------+\n",
      "|Empresa A   |123456780001|95                |\n",
      "|Empresa F   |789321450002|07                |\n",
      "|Empresa B   |987654320001|98                |\n",
      "|Empresa C   |456789120002|36                |\n",
      "|Empresa G   |852963740001|59                |\n",
      "|Empresa H   |963852140001|00                |\n",
      "|Empresa D   |321654980001|39                |\n",
      "|Empresa E   |654123780001|48                |\n",
      "|Empresa I   |741852960003|60                |\n",
      "|Empresa J   |187812030001|28                |\n",
      "+------------+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Usando função UDF\n",
    "# Vantagem: Fácil manutenção e implementação\n",
    "# Desvatagem: Não ser otimizado pelo catalyst, dependendo do volume pode resultar em overhead\n",
    "def calcular_digito_verificador(cnpj):\n",
    "    if not cnpj or len(cnpj) != 12 or not cnpj.isdigit():\n",
    "        return None  # Retorna None se o CNPJ for inválido\n",
    "\n",
    "    # Pesos para o cálculo dos dígitos\n",
    "    dv1 = [5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 2]\n",
    "    dv2 = [6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 2]\n",
    "\n",
    "    # Cálculo do primeiro dígito verficador\n",
    "    soma = sum(int(cnpj[i]) * dv1[i] for i in range(12))\n",
    "    resto = soma % 11\n",
    "    primeiro_digito = 0 if resto < 2 else 11 - resto\n",
    "\n",
    "    # Cálculo do segundo dígito\n",
    "    cnpj_com_primeiro_digito = cnpj + str(primeiro_digito)\n",
    "    soma = sum(int(cnpj_com_primeiro_digito[i]) * dv2[i] for i in range(13))\n",
    "    resto = soma % 11\n",
    "    segundo_digito = 0 if resto < 2 else 11 - resto\n",
    "\n",
    "    return f\"{primeiro_digito}{segundo_digito}\"\n",
    "\n",
    "# Registrando a função como UDF\n",
    "calcular_digito_verificador_udf = udf(calcular_digito_verificador, StringType())\n",
    "\n",
    "\n",
    "# Adicionar coluna com o dígito verificador\n",
    "df = spark.sql(\"SELECT * FROM empresas\")\n",
    "df = df.withColumn(\"digito_verificador\", calcular_digito_verificador_udf(col(\"cnpj\")))\n",
    "\n",
    "# Mostrar o resultado\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6197f7ff-c423-47ac-b5b7-49d6d0ee41aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: Verificar calculo incorreto(script ainda sendo atualizado) - é a melhor alternativa em questão de otimização.\n",
    "# # Calculando o DV1\n",
    "\n",
    "# # Vantagem: Otimizado e ideal para grandes volumes de dados\n",
    "# # Desvantagem: Complexidade\n",
    "\n",
    "# df = spark.sql(\"SELECT * FROM empresas\")\n",
    "# df = df.withColumn(\n",
    "#     \"cnpj_valido\",\n",
    "#     when(length(col(\"cnpj\")) == 12, True).otherwise(False)\n",
    "# )\n",
    "#     # dv1 = [5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 2]\n",
    "#     # dv2 = [6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 2]\n",
    "# # Cálculo do primeiro dígito verificador (DV1)\n",
    "# df = df.withColumn(\n",
    "#     \"DV1\",\n",
    "#     when(\n",
    "#         col(\"cnpj_valido\"),\n",
    "#         (\n",
    "#             substring(col(\"cnpj\"), 1, 1).cast('int') * 5 +\n",
    "#             substring(col(\"cnpj\"), 2, 1).cast('int') * 4 +\n",
    "#             substring(col(\"cnpj\"), 3, 1).cast('int') * 3 +\n",
    "#             substring(col(\"cnpj\"), 4, 1).cast('int') * 2 +\n",
    "#             substring(col(\"cnpj\"), 5, 1).cast('int') * 9 +\n",
    "#             substring(col(\"cnpj\"), 6, 1).cast('int') * 8 +\n",
    "#             substring(col(\"cnpj\"), 7, 1).cast('int') * 7 +\n",
    "#             substring(col(\"cnpj\"), 8, 1).cast('int') * 6 +\n",
    "#             substring(col(\"cnpj\"), 9, 1).cast('int') * 5 +\n",
    "#             substring(col(\"cnpj\"), 10, 1).cast('int') * 4 +\n",
    "#             substring(col(\"cnpj\"), 11, 1).cast('int') * 3 +\n",
    "#             substring(col(\"cnpj\"), 12, 1).cast('int') * 2\n",
    "#         ) % 11\n",
    "#     ).otherwise(None)\n",
    "# )\n",
    "\n",
    "# df = df.withColumn(\n",
    "#     \"DV1\",\n",
    "#     when(col(\"DV1\") >= 10, 0).otherwise(col(\"DV1\"))\n",
    "# )\n",
    "\n",
    "# # Cálculo do segundo dígito verificador (DV2)\n",
    "# df = df.withColumn(\n",
    "#     \"DV2\",\n",
    "#     when(\n",
    "#         col(\"cnpj_valido\"),\n",
    "#         (\n",
    "#             substring(col(\"cnpj\"), 1, 1).cast('int') * 6 +\n",
    "#             substring(col(\"cnpj\"), 2, 1).cast('int') * 5 +\n",
    "#             substring(col(\"cnpj\"), 3, 1).cast('int') * 4 +\n",
    "#             substring(col(\"cnpj\"), 4, 1).cast('int') * 3 +\n",
    "#             substring(col(\"cnpj\"), 5, 1).cast('int') * 2 +\n",
    "#             substring(col(\"cnpj\"), 6, 1).cast('int') * 9 +\n",
    "#             substring(col(\"cnpj\"), 7, 1).cast('int') * 8 +\n",
    "#             substring(col(\"cnpj\"), 8, 1).cast('int') * 7 +\n",
    "#             substring(col(\"cnpj\"), 9, 1).cast('int') * 6 +\n",
    "#             substring(col(\"cnpj\"), 10, 1).cast('int') * 5 +\n",
    "#             substring(col(\"cnpj\"), 11, 1).cast('int') * 4 +\n",
    "#             substring(col(\"cnpj\"), 12, 1).cast('int') * 3 +\n",
    "#             col(\"DV1\").cast('int') * 2\n",
    "#         ) % 11\n",
    "#     ).otherwise(None)\n",
    "# )\n",
    "\n",
    "# df = df.withColumn(\n",
    "#     \"DV2\",\n",
    "#     when(col(\"DV2\") >= 10, 0).otherwise(col(\"DV2\"))\n",
    "# )\n",
    "\n",
    "# # Concatena os dígitos verificadores em uma nova coluna\n",
    "# df = df.withColumn(\n",
    "#     \"digito_verificador\",\n",
    "#     when(col(\"cnpj_valido\"), concat(col(\"DV1\"), col(\"DV2\"))).otherwise(None)\n",
    "# )\n",
    "\n",
    "\n",
    "# df.show()\n",
    "\n",
    "# df.printSchema()\n",
    "# df = df.drop(\"DV1\", \"DV2\", \"cnpj_valido\")\n",
    "\n",
    "# # Mostra o resultado\n",
    "# df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "567a291a-7300-460d-9507-419f6143502b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+---+---+------------------+\n",
      "|nome_empresa|cnpj        |DV1|DV2|digito_verificador|\n",
      "+------------+------------+---+---+------------------+\n",
      "|Empresa A   |123456780001|9  |5  |95                |\n",
      "|Empresa F   |789321450002|0  |7  |07                |\n",
      "|Empresa B   |987654320001|9  |8  |98                |\n",
      "|Empresa C   |456789120002|3  |6  |36                |\n",
      "|Empresa G   |852963740001|5  |9  |59                |\n",
      "|Empresa H   |963852140001|0  |0  |00                |\n",
      "|Empresa D   |321654980001|3  |9  |39                |\n",
      "|Empresa E   |654123780001|4  |8  |48                |\n",
      "|Empresa I   |741852960003|6  |0  |60                |\n",
      "|Empresa J   |187812030001|2  |8  |28                |\n",
      "+------------+------------+---+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Usando enumerate:\n",
    "# Vantagem: Fácil manutenção e implementação, não há serialização(também sendo uma desvatagem)\n",
    "# Desvatagem: Dependendo do volume pode resultar em overhead, pois o dataset é processado dentro da JVM\n",
    "def calcular_digito(cnpj_expr, pesos):\n",
    "    soma = sum(substring(cnpj_expr, i, 1).cast(\"int\") * peso for i, peso in enumerate(pesos, start=1))\n",
    "    resto = soma % 11\n",
    "    return when(resto < 2, 0).otherwise(11 - resto)\n",
    "\n",
    "# DV1 (pesos: 5,4,3,2,9,8,7,6,5,4,3,2)\n",
    "df = spark.sql(\"SELECT * FROM empresas\")\n",
    "df = df.withColumn(\n",
    "    \"DV1\",\n",
    "    calcular_digito(col(\"cnpj\"), [5,4,3,2,9,8,7,6,5,4,3,2])\n",
    ")\n",
    "\n",
    "# DV2 (pesos: 6,5,4,3,2,9,8,7,6,5,4,3,2 + DV1 * 2)\n",
    "df = df.withColumn(\n",
    "    \"DV2\",\n",
    "    calcular_digito(\n",
    "        concat(col(\"cnpj\"), col(\"DV1\")), \n",
    "        [6,5,4,3,2,9,8,7,6,5,4,3,2]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Dígito final\n",
    "df = df.withColumn(\"digito_verificador\", concat(col(\"DV1\"), col(\"DV2\")))\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9be21b97-57cd-4946-baa2-0fd609fd647c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType([StructField('nome_empresa', StringType(), True), StructField('cnpj', StringType(), True), StructField('digito_verificador', StringType(), True)])\n"
     ]
    }
   ],
   "source": [
    "print(df.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8740062-a02d-4555-bf21-0178b4533477",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91621191-773f-4c12-8d10-2c923a1a26d8",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
