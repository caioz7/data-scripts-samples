{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a344b158-535c-45ed-b9fd-b1155eba528c",
   "metadata": {
    "id": "a344b158-535c-45ed-b9fd-b1155eba528c"
   },
   "outputs": [],
   "source": [
    "# !pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336f9a5e-96c0-4519-8ba9-18238c5efb40",
   "metadata": {},
   "source": [
    "# Índice\n",
    "*Notebook contendo apenas algumas funções basicas*\n",
    "\n",
    "1. [Imports](#Import)\n",
    "2. [Usando dropna](#dropna)\n",
    "3. [Dropando duplicados](#dropDuplicates)\n",
    "4. [Função Sample](#sample)\n",
    "5. [Filtrando com filter ou where](#filter_where)\n",
    "6. [Joins](#Join)\n",
    "7. [Agregação(agg)](#agg)\n",
    "8. [Extras](#Extras)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44712dc-4c0a-47ae-8de6-01af4c1c4a32",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8a41a4b0-603f-47d7-970b-fff96e5553e0",
   "metadata": {
    "executionInfo": {
     "elapsed": 376,
     "status": "ok",
     "timestamp": 1740080702647,
     "user": {
      "displayName": "Caio César",
      "userId": "07631910079273269070"
     },
     "user_tz": 180
    },
    "id": "8a41a4b0-603f-47d7-970b-fff96e5553e0"
   },
   "outputs": [],
   "source": [
    "# Importação das bibliotecas necessárias\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, lit, concat, round, substring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5ee7e94-194f-454b-b8e6-fbcb45c0ff08",
   "metadata": {
    "executionInfo": {
     "elapsed": 8472,
     "status": "ok",
     "timestamp": 1740080716139,
     "user": {
      "displayName": "Caio César",
      "userId": "07631910079273269070"
     },
     "user_tz": 180
    },
    "id": "e5ee7e94-194f-454b-b8e6-fbcb45c0ff08"
   },
   "outputs": [],
   "source": [
    "# Inicializar uma sessão do Spark e impedindo avisos desnecessários\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NomeDaApp\") \\\n",
    "    .config(\"spark.logConf\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31d5242-34ae-4896-82c1-e7266a5cee57",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## dropna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93cdf762-bed3-4ea2-b192-5107c7da6b81",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11583,
     "status": "ok",
     "timestamp": 1740080729660,
     "user": {
      "displayName": "Caio César",
      "userId": "07631910079273269070"
     },
     "user_tz": 180
    },
    "id": "93cdf762-bed3-4ea2-b192-5107c7da6b81",
    "outputId": "062ea2c2-3ec0-4373-c4d6-cb4fab36e491"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------+---------+\n",
      "| Nome|Idade|   Cidade|     País|\n",
      "+-----+-----+---------+---------+\n",
      "|Alice|   34|São Paulo|   Brasil|\n",
      "|  Bob|   45|Nova York|      EUA|\n",
      "|Carla|   29|         | Portugal|\n",
      "|  Eva|   25|    Paris|   França|\n",
      "|  Bob|   45|Nova York|      EUA|\n",
      "|Frank|   40|   Tóquio|    Japão|\n",
      "| Hank|   50|   Sydney|Austrália|\n",
      "+-----+-----+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo de DataFrame com 10 registros e 4 campos (incluindo valores nulos/vazios)\n",
    "data = [\n",
    "    (\"Alice\", 34, \"São Paulo\", \"Brasil\"),\n",
    "    (\"Bob\", 45, \"Nova York\", \"EUA\"),\n",
    "    (\"Alice\", None, \"São Paulo\", \"Brasil\"),  # Idade nula\n",
    "    (\"Carla\", 29, \"\", \"Portugal\"),          # Cidade vazia\n",
    "    (\"David\", 30, \"Berlim\", None),          # País nulo\n",
    "    (\"Eva\", 25, \"Paris\", \"França\"),\n",
    "    (\"Bob\", 45, \"Nova York\", \"EUA\"),\n",
    "    (\"Frank\", 40, \"Tóquio\", \"Japão\"),\n",
    "    (\"Grace\", 35, None, \"Reino Unido\"),     # Cidade nula\n",
    "    (\"Hank\", 50, \"Sydney\", \"Austrália\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"Nome\", \"Idade\", \"Cidade\", \"País\"])\n",
    "\n",
    "# Remover linhas com valores nulos ou vazios e salvar na mesma variável `df`\n",
    "df = df.na.drop()  # Remove linhas com qualquer valor nulo ou vazio\n",
    "\n",
    "# Mostrar o resultado\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d024e751-251d-47ab-b6d1-3975462700f4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## dropDuplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "KYdtnett5TdY",
   "metadata": {
    "id": "KYdtnett5TdY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------+-----------+\n",
      "| Nome|Idade|   Cidade|       País|\n",
      "+-----+-----+---------+-----------+\n",
      "|Alice|   34|São Paulo|     Brasil|\n",
      "|  Bob|   45|Nova York|        EUA|\n",
      "|David|   30|   Berlim|   Alemanha|\n",
      "|Carla|   29|   Lisboa|   Portugal|\n",
      "|  Eva|   25|    Paris|     França|\n",
      "|Frank|   40|   Tóquio|      Japão|\n",
      "|Grace|   35|  Londres|Reino Unido|\n",
      "| Hank|   50|   Sydney|  Austrália|\n",
      "+-----+-----+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo de DataFrame com 10 registros e 4 campos\n",
    "data = [\n",
    "    (\"Alice\", 34, \"São Paulo\", \"Brasil\"),\n",
    "    (\"Bob\", 45, \"Nova York\", \"EUA\"),\n",
    "    (\"Alice\", 34, \"São Paulo\", \"Brasil\"),\n",
    "    (\"Carla\", 29, \"Lisboa\", \"Portugal\"),\n",
    "    (\"David\", 30, \"Berlim\", \"Alemanha\"),\n",
    "    (\"Eva\", 25, \"Paris\", \"França\"),\n",
    "    (\"Bob\", 45, \"Nova York\", \"EUA\"),\n",
    "    (\"Frank\", 40, \"Tóquio\", \"Japão\"),\n",
    "    (\"Grace\", 35, \"Londres\", \"Reino Unido\"),\n",
    "    (\"Hank\", 50, \"Sydney\", \"Austrália\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"Nome\", \"Idade\", \"Cidade\", \"País\"])\n",
    "\n",
    "# Remover linhas duplicadas e salvar na mesma variável `df`\n",
    "df = df.dropDuplicates()\n",
    "\n",
    "# Mostrar o resultado\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b912c447-4e6e-45aa-9c20-f1a744a3dfad",
   "metadata": {
    "id": "42c53f2b-5fb8-4bf7-a26e-ea3a8054a796",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dfd6e140-fc95-477b-b2a3-9c233dac1981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------+\n",
      "| Nome|Idade| Cidade|\n",
      "+-----+-----+-------+\n",
      "|  Eva|   28|  Paris|\n",
      "|Frank|   40| Tóquio|\n",
      "|Grace|   33|Londres|\n",
      "+-----+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dados de exemplo\n",
    "data = [\n",
    "    (\"Alice\", 25, \"São Paulo\"),\n",
    "    (\"Bob\", 30, \"Nova York\"),\n",
    "    (\"Carla\", 22, \"Lisboa\"),\n",
    "    (\"David\", 35, \"Berlim\"),\n",
    "    (\"Eva\", 28, \"Paris\"),\n",
    "    (\"Frank\", 40, \"Tóquio\"),\n",
    "    (\"Grace\", 33, \"Londres\"),\n",
    "    (\"Hank\", 50, \"Sydney\"),\n",
    "    (\"Ivy\", 27, \"Toronto\"),\n",
    "    (\"Jack\", 29, \"Cidade do México\")\n",
    "]\n",
    "\n",
    "# Criar DataFrame\n",
    "df = spark.createDataFrame(data, [\"Nome\", \"Idade\", \"Cidade\"])\n",
    "\n",
    "# Aplicar sample para retornar uma amostra aleatória de 30% dos registros\n",
    "df = df.sample(fraction=0.3, seed=None)\n",
    "\n",
    "# Exibir a amostra\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f9c764-9e90-43b1-859c-12b6ffcfa307",
   "metadata": {},
   "source": [
    "## filter_where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "05f9e455-1eb3-414b-8995-85bba7e09c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uso do filter\n",
      "+-----+-----+-------+\n",
      "| Nome|Idade| Cidade|\n",
      "+-----+-----+-------+\n",
      "|David|   35| Berlim|\n",
      "|Frank|   40| Tóquio|\n",
      "|Grace|   33|Londres|\n",
      "| Hank|   50| Sydney|\n",
      "+-----+-----+-------+\n",
      "\n",
      "Uso do where\n",
      "+-----+-----+-------+\n",
      "| Nome|Idade| Cidade|\n",
      "+-----+-----+-------+\n",
      "|David|   35| Berlim|\n",
      "|Frank|   40| Tóquio|\n",
      "|Grace|   33|Londres|\n",
      "| Hank|   50| Sydney|\n",
      "+-----+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dados de exemplo\n",
    "data = [\n",
    "    (\"Alice\", 25, \"São Paulo\"),\n",
    "    (\"Bob\", 30, \"Nova York\"),\n",
    "    (\"Carla\", 22, \"Lisboa\"),\n",
    "    (\"David\", 35, \"Berlim\"),\n",
    "    (\"Eva\", 28, \"Paris\"),\n",
    "    (\"Frank\", 40, \"Tóquio\"),\n",
    "    (\"Grace\", 33, \"Londres\"),\n",
    "    (\"Hank\", 50, \"Sydney\"),\n",
    "    (\"Ivy\", 27, \"Toronto\"),\n",
    "    (\"Jack\", 29, \"Cidade do México\")\n",
    "]\n",
    "\n",
    "# Criar DataFrame\n",
    "df = spark.createDataFrame(data, [\"Nome\", \"Idade\", \"Cidade\"])\n",
    "\n",
    "# Filtra linhas com filter onde a coluna \"Idade\" é maior que 30\n",
    "print(\"Uso do filter\")\n",
    "df.filter(df[\"Idade\"] > 30).show()\n",
    "\n",
    "print(\"Uso do where\")\n",
    "df.where(df[\"Idade\"] > 30).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f9e3b3-2dee-49ec-ad3e-64ebb7175919",
   "metadata": {},
   "source": [
    "## Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2f5a0f88-3b34-4ab2-827d-b1687fcef8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+---------+\n",
      "|id_pessoa| nome|id_cidade|\n",
      "+---------+-----+---------+\n",
      "|        1|Alice|        1|\n",
      "|        2|  Bob|        2|\n",
      "|        3|Carla|        3|\n",
      "|        4|David|        5|\n",
      "+---------+-----+---------+\n",
      "\n",
      "+---------+---------+\n",
      "|id_cidade|   cidade|\n",
      "+---------+---------+\n",
      "|        1|São Paulo|\n",
      "|        2|Nova York|\n",
      "|        3|   Lisboa|\n",
      "|        4|   Berlim|\n",
      "+---------+---------+\n",
      "\n",
      "+---------+-----+---------+\n",
      "|id_pessoa| nome|   cidade|\n",
      "+---------+-----+---------+\n",
      "|        1|Alice|São Paulo|\n",
      "|        2|  Bob|Nova York|\n",
      "|        3|Carla|   Lisboa|\n",
      "+---------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Criando dois dataframes para que possam ser ligados pelo id da cidade\n",
    "data_pessoas = [\n",
    "    (1, \"Alice\", 1),\n",
    "    (2, \"Bob\", 2),\n",
    "    (3, \"Carla\", 3),\n",
    "    (4, \"David\", 5)   \n",
    "]\n",
    "df_pessoas = spark.createDataFrame(data_pessoas, [\"id_pessoa\", \"nome\", \"id_cidade\"])\n",
    "df_pessoas.show()\n",
    "\n",
    "data_cidades = [\n",
    "    (1, \"São Paulo\"),\n",
    "    (2, \"Nova York\"),\n",
    "    (3, \"Lisboa\"),\n",
    "    (4, \"Berlim\")\n",
    "]\n",
    "df_cidades = spark.createDataFrame(data_cidades, [\"id_cidade\", \"cidade\"])\n",
    "df_cidades.show()\n",
    "\n",
    "# Realizando join\n",
    "df_join = df_pessoas.join(df_cidades, df_pessoas.id_cidade == df_cidades.id_cidade, how=\"inner\")\n",
    "# Retornando resultado com select\n",
    "df_join.select(\"id_pessoa\", \"nome\", \"cidade\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7638385-7dc7-4733-b67b-3ce401c00de1",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab48678-8c9d-47bc-b2d3-0e20461295d8",
   "metadata": {},
   "source": [
    "## agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "69de19aa-3938-4478-b2d6-d4f59075f790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+--------------+-------------+\n",
      "|data_abastecimento| trip|gasto_gasolina|consumo_total|\n",
      "+------------------+-----+--------------+-------------+\n",
      "|        2024-11-26|654.0|          38.9|        16.81|\n",
      "|        2024-12-03|602.1|         40.19|        14.98|\n",
      "|        2024-12-18|649.5|          42.8|        15.18|\n",
      "|        2025-01-26|636.9|         42.13|        15.12|\n",
      "+------------------+-----+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dados de exemplo\n",
    "data = [\n",
    "    (\"2024-11-26\", 654.0, 38.9),\n",
    "    (\"2024-12-03\", 602.1, 40.19),\n",
    "    (\"2024-12-18\", 649.5, 42.80),\n",
    "    (\"2025-01-26\", 636.9, 42.13)\n",
    "]\n",
    "\n",
    "# Criar DataFrame\n",
    "df = spark.createDataFrame(data, [\"data_abastecimento\", \"trip\", \"gasto_gasolina\"])\n",
    "\n",
    "# Usar agg para calcular o gasto de combustivel e criar uma nova coluna\n",
    "df = df.withColumn(\"consumo_total\", round(col(\"trip\") / col(\"gasto_gasolina\"), 2))\n",
    "\n",
    "# Exibir o resultado\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf42882e-aaf7-4854-a1d1-1261968749a5",
   "metadata": {
    "id": "d1a0d836-90fb-49f9-8fd0-db8e2a87db40"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b431db-ab6a-482e-b20e-891741004834",
   "metadata": {
    "id": "547fa663-67d9-43be-96af-a13331998467"
   },
   "source": [
    "## Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b790c50-0341-44eb-ba7a-47ebccf71053",
   "metadata": {},
   "source": [
    "### Caracteres Especiais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "409dd5ad-1c46-4ca8-a749-0b59959ee592",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Função para checar caracteres especiais e retornar onde se encontram(util em dataframes grandes)\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col,lit\n",
    "from pyspark.sql.types import ArrayType, StringType, StructType, StructField, TimestampType\n",
    "from pyspark.sql.functions import collect_list, concat_ws\n",
    "\n",
    "def search_dataframe(df, tabela):\n",
    "\n",
    "    intermediate_dfs = []\n",
    "\n",
    "    for coluna in df.columns:\n",
    "\n",
    "        processing_df = df.filter(F.lower(df[coluna]).rlike('[^a-z0-9\\s!`´\"#$%&\\'()*+,-./\\]\\[|:;<=>?@\\[\\\\áàâäãéèêëíìîïóòôöõúùûüñç]'))\n",
    "        processing_df = processing_df.withColumn('COLUMN_ERROR', lit(coluna))\\\n",
    "            .withColumn('TABELA', lit(tabela))\n",
    "        intermediate_dfs.append(processing_df)\n",
    "\n",
    "    # Combine todos os DataFrames intermediários em um único DataFrame\n",
    "    result_df = intermediate_dfs[0]\n",
    "    for i in range(1, len(intermediate_dfs)):\n",
    "        result_df = result_df.union(intermediate_dfs[i])\n",
    "\n",
    "    colunas_referencia = df.columns\n",
    "    # Exiba o DataFrame result_df\n",
    "    # result_df = result_df.groupBy(df.columns,\"TABELA\"]).agg(concat_ws(', ',collect_list(\"COLUMN_ERROR\")).alias('COLUMN_ERROR'))\n",
    "    result_df = result_df.groupBy(*[col(coluna) for coluna in colunas_referencia] + [col(\"TABELA\")]).agg(concat_ws(', ',collect_list(\"COLUMN_ERROR\")).alias('COLUMN_ERROR'))\n",
    "\n",
    "    # Retonando dataframe criado dataframe usado + coluna da tabela + coluna(s) com erros\n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afb4e748-a6a5-481d-93c4-32eb743a3c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------------+-----------+-----+------------+------------+\n",
      "|  Nome|Idade|          Cidade|       País|  Rua|      TABELA|COLUMN_ERROR|\n",
      "+------+-----+----------------+-----------+-----+------------+------------+\n",
      "|Alice☺|   25|       São Paulo|     Brasil|Rua A|tabela_teste|        Nome|\n",
      "| Jack®|   29|Cidade do México|     México|Rua J|tabela_teste|        Nome|\n",
      "|   Bob|   30|      Nova York♠|        EUA|Rua B|tabela_teste|      Cidade|\n",
      "| David|   35|         Berlim∞|   Alemanha|Rua D|tabela_teste|      Cidade|\n",
      "| Frank|   40|         Tóquio☺|      Japão|Rua F|tabela_teste|      Cidade|\n",
      "| Grace|   33|        Londres♠|Reino Unido|Rua G|tabela_teste|      Cidade|\n",
      "| Carla|   22|          Lisboa|  Portugal✓|Rua C|tabela_teste|        País|\n",
      "|  Hank|   50|          Sydney| Austrália✓|Rua H|tabela_teste|        País|\n",
      "+------+-----+----------------+-----------+-----+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (\"Alice☺\", 25, \"São Paulo\", \"Brasil\", \"Rua A\"),\n",
    "    (\"Bob\", 30, \"Nova York♠\", \"EUA\", \"Rua B\"),\n",
    "    (\"Carla\", 22, \"Lisboa\", \"Portugal✓\", \"Rua C\"),\n",
    "    (\"David\", 35, \"Berlim∞\", \"Alemanha\", \"Rua D\"),\n",
    "    (\"Eva\", 28, \"Paris\", \"França\", \"Rua E\"),\n",
    "    (\"Frank\", 40, \"Tóquio☺\", \"Japão\", \"Rua F\"),\n",
    "    (\"Grace\", 33, \"Londres♠\", \"Reino Unido\", \"Rua G\"),\n",
    "    (\"Hank\", 50, \"Sydney\", \"Austrália✓\", \"Rua H\"),\n",
    "    (\"Ivy\", 27, \"Toronto\", \"Canadá\", \"Rua I\"),\n",
    "    (\"Jack®\", 29, \"Cidade do México\", \"México\", \"Rua J\")\n",
    "]\n",
    "\n",
    "# Nomes das colunas\n",
    "colunas = [\"Nome\", \"Idade\", \"Cidade\", \"País\", \"Rua\"]\n",
    "\n",
    "# Criar DataFrame\n",
    "df = spark.createDataFrame(data, colunas)\n",
    "\n",
    "tabela = 'tabela_teste'\n",
    "resultado = search_dataframe(df,tabela)\n",
    "if resultado.isEmpty():\n",
    "    print(f'Sem dados invalidos na tabela {tabela}')\n",
    "else:\n",
    "    resultado.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682ee28c-65fc-4f8e-af05-6935347c9731",
   "metadata": {},
   "source": [
    "### Calculo_CNPJ\n",
    "*Sabemos que os digitos verificadores do CNPJ, são na verdade um calculo dos numeros anteriores, algumas bases antigas usavam essa estrategia para reduzir custos de armazenamento, deixando que funções proprias calculassem exibido na consulta com os digitos verificadores*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "245d8752-42b9-415b-8272-32ba34ff47a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+------------------+\n",
      "|nome_empresa|        cnpj|digito_verificador|\n",
      "+------------+------------+------------------+\n",
      "|   Empresa A|123456780001|                00|\n",
      "|   Empresa F|789321450001|                00|\n",
      "|   Empresa I|741852960001|                00|\n",
      "|   Empresa J|258147360001|                90|\n",
      "|   Empresa G|852963740001|                00|\n",
      "|   Empresa H|963852140001|                78|\n",
      "|   Empresa D|321654980001|                34|\n",
      "|   Empresa E|654123780001|                00|\n",
      "|   Empresa B|987654320001|                12|\n",
      "|   Empresa C|456789120001|                00|\n",
      "+------------+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (\"Empresa A\", \"123456780001\", \"00\"),\n",
    "    (\"Empresa B\", \"987654320001\", \"12\"),\n",
    "    (\"Empresa C\", \"456789120001\", \"00\"),\n",
    "    (\"Empresa D\", \"321654980001\", \"34\"),\n",
    "    (\"Empresa E\", \"654123780001\", \"00\"),\n",
    "    (\"Empresa F\", \"789321450001\", \"00\"),\n",
    "    (\"Empresa G\", \"852963740001\", \"00\"),\n",
    "    (\"Empresa H\", \"963852140001\", \"78\"),\n",
    "    (\"Empresa I\", \"741852960001\", \"00\"),\n",
    "    (\"Empresa J\", \"258147360001\", \"90\")\n",
    "]\n",
    "\n",
    "# Criar DataFrame com a ordem desejada\n",
    "df = spark.createDataFrame(data, [\"nome_empresa\", \"cnpj\", \"digito_verificador\"])\n",
    "\n",
    "tabela=\"empresas\"\n",
    "df.write.mode(\"overwrite\").saveAsTable(tabela)\n",
    "spark.sql(\"SELECT * FROM empresas\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6197f7ff-c423-47ac-b5b7-49d6d0ee41aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculando o primeiro DV\n",
    "# TODO: Verificar calculo incorreto(script desatualizado)\n",
    "try:\n",
    "    df = df.withColumn(\"DV1\", when((col(\"digito_verificador\") == \"00\"), \n",
    "                        ((substring(col(\"cnpj\"), 1, 1).cast('int') * 6) +\n",
    "                        (substring(col(\"cnpj\"), 2, 1).cast('int') * 7) +\n",
    "                        (substring(col(\"cnpj\"), 3, 1).cast('int') * 8) +\n",
    "                        (substring(col(\"cnpj\"), 4, 1).cast('int') * 9) +\n",
    "                        (substring(col(\"cnpj\"), 5, 1).cast('int') * 2) +\n",
    "                        (substring(col(\"cnpj\"), 6, 1).cast('int') * 3) +\n",
    "                        (substring(col(\"cnpj\"), 7, 1).cast('int') * 4) +\n",
    "                        (substring(col(\"cnpj\"), 8, 1).cast('int') * 5) +\n",
    "                        (substring(col(\"cnpj\"), 9, 1).cast('int') * 6) +\n",
    "                        (substring(col(\"cnpj\"), 10, 1).cast('int') * 7) +\n",
    "                        (substring(col(\"cnpj\"), 11, 1).cast('int') * 8) +\n",
    "                        (substring(col(\"cnpj\"), 12, 1).cast('int') * 9)) % 11).otherwise(''))\n",
    "    df = df.withColumn('DV1', when(df.DV1 >= '10', '0').otherwise(col('DV1')))\n",
    "except Exception as e:\n",
    "    print(str(e))\n",
    "\n",
    "# Calculando o segundo DV\n",
    "try:\n",
    "    df = df.withColumn(\"DV2\", when((col(\"digito_verificador\") == \"00\"), \n",
    "                        ((substring(col(\"cnpj\"), 1, 1).cast('int') * 5) +\n",
    "                        (substring(col(\"cnpj\"), 2, 1).cast('int') * 6) +\n",
    "                        (substring(col(\"cnpj\"), 3, 1).cast('int') * 7) +\n",
    "                        (substring(col(\"cnpj\"), 4, 1).cast('int') * 8) +\n",
    "                        (substring(col(\"cnpj\"), 5, 1).cast('int') * 9) +\n",
    "                        (substring(col(\"cnpj\"), 6, 1).cast('int') * 2) +\n",
    "                        (substring(col(\"cnpj\"), 7, 1).cast('int') * 3) +\n",
    "                        (substring(col(\"cnpj\"), 8, 1).cast('int') * 4) +\n",
    "                        (substring(col(\"cnpj\"), 9, 1).cast('int') * 5) +\n",
    "                        (substring(col(\"cnpj\"), 10, 1).cast('int') * 6) +\n",
    "                        (substring(col(\"cnpj\"), 11, 1).cast('int') * 7) +\n",
    "                        (substring(col(\"cnpj\"), 12, 1).cast('int') * 8) +\n",
    "                        (col('DV1').cast('int') * 9)) % 11).otherwise(''))\n",
    "    df = df.withColumn('DV2', when(df.DV2 >= '10', '0').otherwise(col('DV2')))\n",
    "except Exception as e:\n",
    "    print(str(e))\n",
    "\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f8740062-a02d-4555-bf21-0178b4533477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+------------------+---+---+\n",
      "|nome_empresa|        cnpj|digito_verificador|DV1|DV2|\n",
      "+------------+------------+------------------+---+---+\n",
      "|   Empresa A|123456780001|                00|  0|  1|\n",
      "|   Empresa B|987654320001|                12|   |   |\n",
      "|   Empresa C|456789120001|                00|  0|  0|\n",
      "|   Empresa D|321654980001|                34|   |   |\n",
      "|   Empresa E|654123780001|                00|  0|  0|\n",
      "|   Empresa F|789321450001|                00|  1|  0|\n",
      "|   Empresa G|852963740001|                00|  0|  0|\n",
      "|   Empresa H|963852140001|                78|   |   |\n",
      "|   Empresa I|741852960001|                00|  0|  0|\n",
      "|   Empresa J|258147360001|                90|   |   |\n",
      "+------------+------------+------------------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91621191-773f-4c12-8d10-2c923a1a26d8",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
